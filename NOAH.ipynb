{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2mP8d0pT8DCU"},"outputs":[],"source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = \"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\"  # replace with your real key\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348348,"status":"ok","timestamp":1755209009913,"user":{"displayName":"joe williams","userId":"05243360962752849460"},"user_tz":300},"id":"QpY2zCk28my-","outputId":"badfb102-5890-4464-f5f3-13d9d6ccb3bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip -q install -U pydantic httpx llama-cpp-python\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXSyusRO8sYH","executionInfo":{"status":"ok","timestamp":1755452002914,"user_tz":300,"elapsed":53,"user":{"displayName":"joe williams","userId":"05243360962752849460"}},"outputId":"415d95ab-efbd-4a28-d7fe-4c82ede3f83e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Nova minimal package written.\n"]}],"source":["# Build a minimal Nova package so Cells 4 & 5 work\n","import os, textwrap, json, time\n","\n","# dirs\n","os.makedirs(\"nova/agents\", exist_ok=True)\n","os.makedirs(\"nova/memory\", exist_ok=True)\n","os.makedirs(\"models\", exist_ok=True)\n","\n","# __init__.py\n","open(\"nova/__init__.py\",\"w\").write(\"from .config import Settings\\nsettings = Settings()\\n\")\n","\n","# config.py\n","open(\"nova/config.py\",\"w\").write(textwrap.dedent(\"\"\"\n","from pydantic import BaseModel\n","import os\n","class Settings(BaseModel):\n","    llm_backend: str = os.getenv(\"NOVA_LLM_BACKEND\",\"openai\")\n","    openai_model: str = os.getenv(\"NOVA_OPENAI_MODEL\",\"gpt-4o-mini\")\n","    request_timeout: int = int(os.getenv(\"NOVA_TIMEOUT\",\"60\"))\n","\"\"\"))\n","\n","# llm.py (OpenAI path only for now)\n","open(\"nova/llm.py\",\"w\").write(textwrap.dedent(\"\"\"\n","import os, httpx\n","from .config import Settings\n","settings = Settings()\n","\n","def chat(prompt: str) -> str:\n","    if settings.llm_backend != \"openai\":\n","        return \"Backend not set to openai; set NOVA_LLM_BACKEND=openai.\"\n","    key = os.getenv(\"OPENAI_API_KEY\")\n","    if not key:\n","        return \"OPENAI_API_KEY not set.\"\n","    headers={\"Authorization\":f\"Bearer {key}\",\"Content-Type\":\"application/json\"}\n","    data={\"model\":settings.openai_model,\"messages\":[{\"role\":\"user\",\"content\":prompt}]}\n","    with httpx.Client(timeout=settings.request_timeout) as c:\n","        r=c.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n","        r.raise_for_status()\n","        return r.json()[\"choices\"][0][\"message\"][\"content\"]\n","\"\"\"))\n","\n","# profiles.py\n","open(\"nova/profiles.py\",\"w\").write(textwrap.dedent(\"\"\"\n","def apply_style(user_prompt: str) -> str:\n","    prefix = (\n","        \"System: You are NOVA, a disciplined build/assistant.\\\\n\"\n","        \"Be concise, fix errors, show steps. If code is broken, propose a minimal working example.\\\\n\"\n","        \"User: \"\n","    )\n","    return prefix + user_prompt\n","\"\"\"))\n","\n","# memory/log.py\n","open(\"nova/memory/log.py\",\"w\").write(textwrap.dedent(\"\"\"\n","import os, json, time\n","LOG_PATH = os.getenv(\"NOVA_LOG_PATH\",\"./nova_memory.jsonl\")\n","def log_event(kind: str, data: dict):\n","    entry = {\"ts\": time.time(), \"kind\": kind, \"data\": data}\n","    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n","        f.write(json.dumps(entry, ensure_ascii=False) + \"\\\\n\")\n","    return entry\n","\"\"\"))\n","\n","# agents/scribe.py\n","open(\"nova/agents/scribe.py\",\"w\").write(textwrap.dedent(\"\"\"\n","from ..memory.log import log_event\n","def summarize_and_log(title: str, notes: str):\n","    return log_event(\"codex\", {\"title\": title, \"notes\": notes})\n","\"\"\"))\n","\n","# agents/outreach.py\n","open(\"nova/agents/outreach.py\",\"w\").write(textwrap.dedent(\"\"\"\n","def draft(goal: str) -> str:\n","    return f\"Subject: Quick Win\\\\n\\\\nShort version—{goal}.\\\\nIf it makes sense, I'll keep it simple and do the heavy lifting.\\\\n\\\\n— Joe (McDona Holdings)\"\n","\"\"\"))\n","\n","print(\"Nova minimal package written.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1755209018632,"user":{"displayName":"joe williams","userId":"05243360962752849460"},"user_tz":300},"id":"4DGU4gSK8s36","outputId":"64594627-37a8-43d7-826f-e656460d36fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logged to nova_memory.jsonl\n"]}],"source":["from nova.agents.scribe import summarize_and_log\n","\n","summarize_and_log('Nova Repair Complete', 'Core cells executed and backend verified.')\n","print(\"Logged to nova_memory.jsonl\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1755209019684,"user":{"displayName":"joe williams","userId":"05243360962752849460"},"user_tz":300},"id":"ieUOU9n086Wr","outputId":"07d96f67-dc51-4df8-9830-05c8f65c79db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Subject: Quick Win\n","\n","Short version—Intro call to discuss a quick win for your ops.\n","If it makes sense, I'll keep it simple and do the heavy lifting.\n","\n","— Joe (McDona Holdings)\n"]}],"source":["from nova.agents.outreach import draft\n","print(draft(\"Intro call to discuss a quick win for your ops\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1755209020948,"user":{"displayName":"joe williams","userId":"05243360962752849460"},"user_tz":300},"id":"-YoNuGa7A41i","outputId":"7789d405-c655-4652-dd70-b6111f3bca5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","📂 All Nova + Noah files indexed into nova_memory.json\n","\n","🤖 Agent Mode Active — 2025-08-14 22:03:42\n","Nova + Noah files loaded. Awaiting commands...\n"]}],"source":["import os\n","import glob\n","import json\n","\n","# === 1. LOAD ALL NOVA + NOAH FILES INTO MEMORY ===\n","nova_dir = \"/content\"  # Change if your PDFs are in a subfolder\n","pdf_files = sorted(glob.glob(os.path.join(nova_dir, \"Nova_Pack*.pdf\"))) + sorted(glob.glob(os.path.join(nova_dir, \"*Noah*.pdf\")))\n","\n","loaded_docs = []\n","\n","for pdf_path in pdf_files:\n","    try:\n","        with open(pdf_path, \"rb\") as f:\n","            loaded_docs.append({\n","                \"filename\": os.path.basename(pdf_path),\n","                \"content\": f.read()\n","            })\n","        print(f\"✅ Loaded: {os.path.basename(pdf_path)}\")\n","    except Exception as e:\n","        print(f\"❌ Error loading {pdf_path}: {e}\")\n","\n","# Save a memory index\n","with open(\"nova_memory.json\", \"w\") as f:\n","    json.dump({\"files_loaded\": [doc[\"filename\"] for doc in loaded_docs]}, f, indent=2)\n","\n","print(\"\\n📂 All Nova + Noah files indexed into nova_memory.json\")\n","\n","# === 2. AGENT MODE STARTER ===\n","from datetime import datetime\n","\n","def agent_mode():\n","    print(\"\\n🤖 Agent Mode Active —\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n","    print(\"Nova + Noah files loaded. Awaiting commands...\")\n","    # Add your command loop or AI function calls here\n","    # Example: respond_to_query(\"Summarize Noah starter content\")\n","\n","agent_mode()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_0gqZfZLBYj"},"outputs":[],"source":["def respond_to_query(query: str) -> str:\n","    query = query.strip()\n","    styled = apply_style(query)\n","\n","    for attempt in range(5):\n","        try:\n","            raw = chat(styled)\n","            return _clean_model_text(raw)\n","        except HTTPStatusError as e:\n","            if e.response is not None and e.response.status_code == 429:\n","                print(\"⚠️ Rate-limited (429). Waiting 60 seconds before retry...\")\n","                sleep(60)  # Wait a full minute\n","                continue\n","            raise\n","    return \"Still hitting rate limits—try again later.\"\n","\n","\n","\n","def _clean_model_text(text: str) -> str:\n","    lines = text.splitlines()\n","    drop_prefixes = [\"system:\", \"user:\", \"assistant:\", \"you:\", \"nova:\"]\n","    cleaned = [\n","        l for l in lines\n","        if not any(l.strip().lower().startswith(p) for p in drop_prefixes)\n","    ]\n","    # Collapse multiple blank lines\n","    return \"\\n\".join(l for l in cleaned if l.strip() != \"\").strip()\n","\n","def respond_to_query(query: str) -> str:\n","    query = query.strip()\n","    styled = apply_style(query)\n","    raw = chat(styled)\n","    return _clean_model_text(raw)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1755209026299,"user":{"displayName":"joe williams","userId":"05243360962752849460"},"user_tz":300},"id":"AMnXwK6yP9u0","outputId":"69039513-9329-4c70-ee97-c819681f6d7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Has key: True\n","Prefix: sk-proj\n"]}],"source":["import os\n","\n","# Check if key already exists\n","print(\"Has key:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n","print(\"Prefix:\", (os.getenv(\"OPENAI_API_KEY\") or \"\")[:7])  # should start with sk-proj\n","\n","# If no key or wrong one, set it here:\n","os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1755209027740,"user":{"displayName":"joe williams","userId":"05243360962752849460"},"user_tz":300},"id":"XtyYueZRQgL1","outputId":"5d9376af-7c0b-472a-ff72-4ed67f8798a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["API key set: True\n"]}],"source":["import os\n","\n","# Set your API key here before running the loop\n","os.environ[\"OPENAI_API_KEY\"] = \"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\"\n","\n","print(\"API key set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"]},{"cell_type":"markdown","metadata":{"id":"4gGRv1mPVXVB"},"source":["# --- Build a tiny PDF memory/index from your packs ---\n","!pip -q install pymupdf\n","\n","import os, glob, math\n","import fitz  # PyMuPDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def read_pdf_text(path):\n","    doc = fitz.open(path)\n","    pages = []\n","    for i in range(len(doc)):\n","        txt = doc[i].get_text(\"text\")\n","        if txt:\n","            pages.append(txt)\n","    doc.close()\n","    return \"\\n\".join(pages)\n","\n","# Grab your packs (adjust patterns if your filenames differ)\n","PDF_PATHS = sorted(glob.glob(\"Nova_Pack*.pdf\")) + sorted(glob.glob(\"Noah_Pack*.pdf\"))\n","assert PDF_PATHS, \"No Nova/Noah PDF packs found in this session. Upload them to the left panel first.\"\n","\n","# Chunk the text so retrieval works nicely\n","def chunk(text, max_chars=1200, overlap=200):\n","    out, i = [], 0\n","    while i < len(text):\n","        out.append(text[i:i+max_chars])\n","        i += max_chars - overlap\n","    return [c.strip() for c in out if c.strip()]\n","\n","DOCS = []\n","META = []\n","for p in PDF_PATHS:\n","    full = read_pdf_text(p)\n","    for idx, c in enumerate(chunk(full)):\n","        DOCS.append(c)\n","        META.append({\"source\": os.path.basename(p), \"chunk\": idx})\n","\n","# Build TF-IDF index\n","VECT = TfidfVectorizer(stop_words=\"english\")\n","MATRIX = VECT.fit_transform(DOCS)\n","\n","print(f\"Loaded {len(PDF_PATHS)} PDFs → {len(DOCS)} chunks indexed.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2hLXIISVlzZ"},"outputs":[],"source":["from nova.llm import chat\n","from nova.profiles import apply_style\n","import re\n","from time import sleep\n","from httpx import HTTPStatusError\n","import numpy as np\n","\n","def _clean_model_text(text: str) -> str:\n","    lines = text.splitlines()\n","    drop = (\"System:\", \"User:\", \"Assistant:\", \"You:\", \"Nova:\")\n","    keep = [ln for ln in lines if not any(ln.strip().lower().startswith(p.lower()) for p in drop)]\n","    # collapse blanks\n","    keep = [ln for ln in keep if ln.strip() != \"\"]\n","    return \"\\n\".join(keep).strip()\n","\n","def _retrieve(query: str, k: int = 6):\n","    if not DOCS:\n","        return []\n","    qv = VECT.transform([query])\n","    sims = cosine_similarity(qv, MATRIX).ravel()\n","    top_idx = np.argsort(-sims)[:k]\n","    return [(DOCS[i], META[i], float(sims[i])) for i in top_idx]\n","\n","def respond_to_query(query: str) -> str:\n","    query = query.strip()\n","    ctxs = _retrieve(query, k=6)\n","\n","    if ctxs:\n","        context_block = \"\\n\\n\".join(\n","            [f\"[{m['source']} #{m['chunk']}] {c}\" for (c, m, s) in ctxs]\n","        )\n","        user_prompt = (\n","            f\"Use ONLY the context below to answer the user. \"\n","            f\"If info is missing, say so briefly.\\n\\n\"\n","            f\"=== Context ===\\n{context_block}\\n\\n\"\n","            f\"=== Task ===\\n{query}\"\n","        )\n","    else:\n","        user_prompt = query  # fallback\n","\n","    styled = apply_style(user_prompt)\n","\n","    # Gentle retry for brief rate limits\n","    delay = 1.5\n","    for _ in range(4):\n","        try:\n","            raw = chat(styled)\n","            return _clean_model_text(raw)\n","        except HTTPStatusError as e:\n","            if getattr(e, \"response\", None) and getattr(e.response, \"status_code\", None) == 429:\n","                print(f\"⚠️ Rate-limited; retrying in {delay:.1f}s…\")\n","                sleep(delay)\n","                delay *= 2\n","                continue\n","            raise\n","    return \"Still hitting rate limits — try again in a minute.\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLQue6iZBpUo"},"outputs":[],"source":["while True:\n","    user_input = input(\"🗣️ Your Command: \")\n","    if user_input.lower() in [\"exit\", \"quit\"]:\n","        print(\"👋 Exiting Agent Mode...\")\n","        break\n","    response = respond_to_query(user_input)\n","    print(f\"🤖 Nova+Noah: {response}\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1rKwl85PXVfFiAOVdSZDdVVar3hXzj6L0","timestamp":1755454982643}],"authorship_tag":"ABX9TyNfJ008ma3GLjf4tPOg7Lcc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}